https://www.reddit.com/r/sysadmin/comments/85l69j/cable_storage/

https://avtech.com

https://kentix.com/en/

https://www.reddit.com/r/sysadmin/comments/84z22i/datacentre_storage_requirements/

https://www.reddit.com/r/sysadmin/comments/16m9pq/setting_up_a_data_center_what_do_you_need_to_know/

https://www.reddit.com/r/sysadmin/comments/7x2ow9/new_it_room_supplies/

#Build or redesign a server room
Maybe you are moving from one location to another location and its time to build a new server room

http://www.channelpronetwork.com/article/building-perfect-server-closet
https://forums.servethehome.com/index.php?threads/living-the-dream-building-our-own-server-room-from-scratch.3079/
https://community.spiceworks.com/how_to/2622-how-to-build-or-redesign-a-server-room
https://www.cio.com/article/2382635/small-business/small-business-6-steps-for-setting-up-a-small-business-server-room.html
https://www.wikihow.com/Design-a-Server-Room

The right server room doesn't exist it all depends on what the demands are
You could have various approaches on building the server room for your company needs. Various approaches 

Depeninding on the ammount of servers you can hav



#### Equipment
* Anti static wipes
* Bar-code scanner if you're leveraging bar codes in any way
* Batteries 
* Cable organizer straps
* Console cable
* Dust-off compressed air
* Ear protection for prolonged work in the area
* External HDD's (3TB or better).
* Flashlight Head LED
* Flashlight LED
* KVM station (Monitor/Keyboard/Mouse) for staging/troubleshooting / crash cart
* Label maker / printer
* Serial cable
* Spindle with blank DVD-Rs or CDR's
* USB Drives 4gb / 8gb
* Zip ties
* UPSs
* PDUs
* UTP punch-down tool

#### Uninterruptible Power Supply
* UPS batteries need to be replaced every 3-5 years
* Calculator for UPS, for example: http://www.apc.com/tools/ups_selector/index.cfm

#### Handy information
* Make sure doors are large enough for all equipment. ( eg. Racks )
* Clear path to the loading docks\service elevator
* Raised floors but make sure it can hold the weight

#### Cooling
* Dual air-con with failover, automatic turn on after power failure.

#### Others
* Sensory for leak detection ( for cooling units)
* Sensors for humidity, water, smoke and temperature sensing
* Workbench for server build and repair
* Shelving
* Storage
* Make sure you have an empty containers 

#### Room information
* There may be no water sources above the server room. E.g. sinks / toilets, water pipes. 
* Depending on your location you need to make sure the server room is above sea level so there is no risk of a flood.
* Completely separate eletrical circuits to the rest of the building.


5x each - 1 foot, 5 foot, 10 foot, 15 foot Cat5e cables, snagless
SATA/ATA to USB dock for both laptop and standard size drives

Small cordless screwdriver
PC repair toolkit (these are a dime a dozen, just something with lots of screwdrivers, the basics)
LED Flashlight

velcro cable organizer straps
4-5 machine USB/Display KVM
8 port POE Gigabit Switch


Shelving, storage, containers (usually bins).
You want a proper UTP tester, not a simplistic toner. Either a tester or "toner" for fiber as well.
Today, the ones that are USB connected seem more versatile and faster to use than handheld.

UTP punch-down tool.

Bench with abundant power, if you don't already have an area for this.
Safe for high security material and on-site backup media.
Digital multimeter, if anyone can be expected to know how to use it.
Every so often a temperature-controlled soldering station can be a life saver, again, if someone knows how to use it.
Heat gun if you use heat-shrink tubing.

Industrial wire shelving and bins


Multimeter
Rolls of velcro
Screwdrivers and sockets
Tool chest
Cable termination gear: crimper, punchdown, etc
Magnetic bowls
Pliers/needle-nose
Good scissors/sharp knife


Fluke LinkRunner AT.
Extra patch cords.
Good set of tools (crimpers, 110 punch down tool).
Couple rolls of velcro strapping.

Painters tape.
Black and Silver sharpies.

Notebooks and clip boards.
Two large whiteboards and a smaller one for each person.
Dry erase markers.
1 inch binders with document protectors for critical documentation.
USB drives for backing up documentation.
A good UPS system for servers/switches and IT desktops.
Flashlights for when (not if) the power goes out.
Extra standard power cords.
Extra laptop power packs,
Extra common cables such as micro-usb, mini-usb, type-c, lightning, old style apple, usb type A to B, etc.
Stapler (red swingline for bonus geek points).

Low velocity vacuum.
Each IT person needs at least two monitors and a set of decent speakers.
Extra keyboards and mice.
Black enamel paint for when you decide to "blackout" your coworkers keyboard when they are on vacation.
Blank DVD-R's and CDR's (NOT the RW's though).


Backup cooling. If a fully redundant cooling unit is beyond your budget, a "dalek", or even a high-powered fan and venting duct is better than nothing. You could also look at buying store credit at a local rental place - don't rely on management to keep money set aside.
Wall loading. It's never fun when a loaded wall-mount rack rips itself off the wall because some fool mounted it onto the drywall, or onto those weak-ass aluminium studs.
Floor loading. Make sure your floor can take the weight, including in the hall you're moving the kit through.
Service contracts. ACs and UPSs need regular maintenance, don't neglect it!
TOIL or overtime agreements. Make sure you get reimbursed for any out-of-hours migration work.


Water, smoke and temperature sensing.
Personally I'd recommend a floor rack rather wall mount. If you DO go wall mount, I always preferred a giant sheet of plywood. My last gig, it was on a block wall so when I was done, you could probably have mounted a motorcycle by the time you were done. Personally, I paint it before or after mounting. Makes it much easier to mount later stuff and is less messy, but I'm quirky on neat cabling and mounting when humanly possible.
Two power poles. One to UPS, other to normal power or ideally second UPS. If not a second UPS, good sacrificial surge protector. Make sure it is on its own breaker. Ideally, get a dedicated panel for the room. Makes life easier if you can swing a generator. If not a permanent installed generator, make the provisions for a cutover to route your dedicated panel to a generator without backfeeding. That would be very bad. Not ueber expensive, most of those have been $600-$1500 depending on the circumstances.
Lockable door. I swung for combo locks with an audit trail, but a camera or two that you can remotely access would do the job fine.
FLASHLIGHTS. BIG ONES. Buy or make mounts, put them on the walls. Spare batteries.
Place for stores. I just bought cheap clear plastic bins and a label maker. Every bloody thing goes in a plastic bin, bin is labeled and then neatly shelved. Best system I've figured out for storing... well, everything that wasn't hideously expensive, easy to follow and actually helpful.
Take a couple minutes to ponder your fire suppression. If you can't afford a real gas or dry water system, make provisions and get it put in some future budget. If nothing else, make sure the fire extinguishers are electronics safe.
All critical keys on a key ring in a specific location. Something you can grab in seconds.
Sort out your cable management. Overhead wire troughs aren't very expensive and can be managed easily with velcro or zip ties. Small ladder makes life easier.
Small tool box with cheap tools so you're not running around for when you need a specific sized flat head or whatnot.

The rail kits always make me feel like an idiot. I think the best thing to do is lay the server on a bench and figure out how the inserts slide into the rails. Then take it all apart and mount it in the rack.



Everyone here was bad at these tasks until we got good through trial & error.

Everyone here has a story of when they installed some cables, stepped back to look at their work, and decided they HAD to uninstall them, and do it again a second time, a different way.

What type of network cables should I buy, and how many?

Apply your diagnostic & problem solving skills and develop your own answer to this question.

What kind of NIC is in the server?
RJ45 or SFP+ ?
What kind of switch are you connecting to?
Does it have RJ45 ports or SFP+ ports?
How fast do the NIC & switch go?
CAT-5e is certified for up to 1Gbps of capacity.
CAT-6 is certified for 1Gbps out to 100m length, and 10Gbps out to 55m.
CAT-6A is certified for 1Gbps or 10Gbps out to 100m length.
SFP+ Twinax or DAC (Direct Attach Cables) support 10Gbps out to about 15m.
SFP+ Fiber Optic connections can go much longer distances.
How many cables should you buy?
- How many do you need, plus how many spared make you feel confident & comfortable.
- MAKE SURE you are buying the correct cable lengths.

And also - I need SAN cables for the EMC.
What kinds of connectors does your SAN provide?
What kind of connector does the FC HBA in the server provide? (I'll bet it's an LC connector)


Cat6 is your friend. It's cheap to install now, VERY EXPENSIVE to install later. Figure out the maximum amount of Cat6 you think you might ever need, then double that.

At the very least, each area that might possibly someday be used as a desk should have 2 Cat6's.

Consider bundle cable. That's where a few separate wires are bundled together in one fat wire. In a home you'd commonly find 2 Cat6 and 2 coax. However there are lots of choices, maybe 2x Cat6 and 1 run of 50 micron fiber would be good.

Conduit is also your friend, especially if you won't have drop ceilings.

Remember WiFi! Have Cat6 runs (two) to areas in the ceiling where your WiFi APs will go. I recommend Ubiquiti UAP-AC-PRO's (802.11ac centrally managed PoE AP for $150, NOT consumer garbage) but if you want to pay $400/AP for Cisco then have fun. wavvo had a good idea of doing a wireless survey, although I believe in just putting one AP in each room if you don't have a stupid number of rooms.

Cooling and power for your server room depend on what you'll be running there. If it's just a few servers in one rack, an exhaust fan or a Mr. Slim might be all you need. If you'll have several racks worth of stuff, then talk to someone who's familiar with DATACENTER cooling. Long story short, the server cooling system should ideally be separate from the rest of the building, because you're going to have a lot of heat in one room.

You'll also want environmental monitoring equipment in there. While stories like this may be an extreme 'it could happen', you still want to know if/when the AC starts to fail.

Same thing with power- if it's just one or two racks then maybe a few 240v dedicated circuits are all you need, but if it's bigger you'll need more. Consider UPS- if you have more than a couple racks it can make sense to get a larger UPS (think Symmetra).

Fire is another issue. A datacenter bigger than 1-2 racks may need its own fire suppression. You don't want sprinklers for reasons that should be obvious. That means halon type systems.

Also consider physical security. The datacenter should not be accessible with standard 'master keys'- no janitor or night security guard should be able to get into the server room. There is literally no benefit to having such access and lots of potential downsides. If the building is going to have electronic locks (which any good building should), you should be all over the security vendor and carefully integrating their equipment into your network. You (yes YOU, the IT person) should have management access to that system IMHO and it should not be remotely accessible (or at least the part that controls the datacenter shouldn't be).

Also- try to keep major plumbing runs away from the datacenter. You don't want servers to die because a sewer pipe in the ceiling above it sprung a leak...


We did a new datacenter build out last year. A few things I really thought were helpful.

Buy as big (deep) of a rack as you can. I think we did the apc 1200mm racks. Plenty of room in the back for pdus and cable management.

We did not do raised floor. We either had top of rack switching or used cisco fex so the majority of runs were in the same cabinet.

We did put in cable trays with waterfalls for fiber cables.

Labeling everything!!!! We had a Brady tls label maker cables were labeled on each end with origin and destination. All power cables were labeled as well

While your adding power cables add them to the networked pdus as well. If you don’t do it while your adding them they will likely never get updated correctly.

Right length cables. Try as best you can to measure out your runs and buy the correct length. Nothing worse than using a 20 foot cable for a 6 foot run.

We did in row air conditioning. It works I have no feelings for or against it. But it did take up two rack location.

Buy a bunch of rolls of cable Velcro. Use that and only that. Anyone using zip ties slap them.
For any cable that goes into a conduit of any sort, it gets labeled or tagged every 3-6 feet. That way if you have to trace a bad cable it's easier to keep track of which one you're testing. Seems wasteful at first, but you'll be glad you did it when bad things happen.

https://www.servertech.com/families/ci22?product=STV-4103D


Cooling

Adequate cooling for the room size.
Adequate cooling for the amount of equipment within the room.
Redundancy within the air-conditioning systems. If the building air-conditioning fails, or we have an issue with one unit – will the room remain cooled?
Front-to-back cooling – we should have the warm air from the back of the racks being exhausted outwards, and the front of the racks should be supplied with cool air from air conditioning units.
Condenser must be located outside of the server room, otherwise we have a potential water source if it fails. As below.
Power

Adequate power availability for all the equipment.
Separated power circuits from the rest of the building. If the room next door trips, we do not want the server room failing over to UPS.
Fusebox / breakers for the server room located away. If there is an issue in the room, we do not want to have to enter the room in an emergency to shut off the power.
The server room must be able to be automatically connected with the feed from the generator in the event of a power outage.
Fire / Smoke

Smoke detectors must be available within the room.
Is there consideration for a suppression system? We’re looking at nearly 10 racks of equipment combined, and that’s a large fire hazard that will go up quickly if the worst happens.
There must not be any water-based sprinkler system in the room whatsoever.
Electrical fire extinguishers must be available here.
Environmental

Are there any environmental monitoring systems being considered? Humidity and temperature are critical as these will cause the equipment to have a significantly shortened lifespan.
There must be no water sources above the server room. E.g. sinks / toilets, water pipes running above the ceiling.
Is the room high enough above sea level that there is no risk of the room ever flooding?

REDUDANT COOLING I CANNOT STRESS THIS ENOUGH.


Completely separate eletrical circuits to the rest of building.
Don't forget about additional 15AMP line for UPS etc
Raised floor.
Dual air-con with failover, automatic turn on after power failure.
Environmental monitoring.
Swipe/Keycard access.
Plenty of room for racks which are located in easily accessible part of the room.
Extra room for workspace/shelving/cupboards.

KVM and crash cart.
Workbench for server build and repair
Make sure doors are large enough for all equipment. ( eg. Racks )
And a clear path to the loading docks\service elevator.




You also don't mention room size - how much space do you have to play with? Generally, you want your equipment racks mostly situated so that you have good working room on all sides - you can mount your racks/enclosures next to each other, but make sure to have ample room forward, back, and to the sides of the cluster to easily move equipment around. Think really heavy servers on a work cart sort of movement if you have room. If not, then for FSM's sake, at least leave yourself good room forward and aft. I've seen people leave themselves six inches behind the rack once the doors were open and they always have to do a weird dance to get in to check things out.

Think of your cable infrastructure both to the racks/enclosures and INSIDE the racks and enclosures. Get cable managment (both horizontal and vertical) and cruise /r/cableporn for ideas on what to do and what not to do. Personally, I'm a fan of the TE Connectivity products - I use 2U horizontal cable managers like this where I can because it really cleans up a rack and can make patching and re-patching a lot quicker and more likely to STAY clean because of that. Black Box and Panduit make decent stuff as well, but I just like the TE connectivity equipment more.

If you have the space, try and reserve a rack section for network and reserve some extra panel space for growth if the company anticipates it in the next 5-20 years. Depending on situation I like to centralize power and network in one rack and servers in the other, or distribute it a bit so network patch panels are in one rack, but there's a switch and UPS at least in each rack/enclosure. This keeps your cabling a lot neater and easier to work with.

Get some good, managed UPSs. Get an environmental sensor card or something like a WeatherGoos/WatchDog. There are cheaper products out there like the AvTech line, but you want something that will write logs so that you can look at historical information and trends in a simple interface.

Workspace in the server room I'm kinda conflicted on - it can be handy in a rush, but generally I prefer to keep work and clutter out of the server room. Less chance of accidents. Same with storage - it can be a fight to keep the server room from becoming a general storage room, so maybe one cabinet for some server room supplies, but general IT stuff goes elsewhere.

The KVM and crash cart really depends on the size of your infrastructure. I simple rack-mounted KVM may suffice.


Here's APC's calculator for UPS.
http://www.apc.com/tools/ups_selector/index.cfm



Overlap between when your colo contract runs through and when your server room is ready
Leak detection (for cooling units)
Temp/humidity sensors
ATS
Genset fuel tanks
UPSs
PDUs
All of this requires regular maintenance
UPS batteries need to be replaced every 3-5 years
Your ISP is going to charge you high transport costs to build into your building
Converting to dry pipe fire suppression is actually pretty expensive. Adding something gas based more so.

Off-site backups.






Just did this project this summer:

Redundant AC units, where one unit can cool the whole room, in case of failure or for maintenance. Liebert and APC make full rack sized cooling units for this purpose.

I always run power overhead as well. I don't care where you are, there's always a chance of water making it into the room.

Speaking of which, water sensors under the raised floor. It can be hard to hear water under the floor, and you don't want to find out there's a problem as the water comes through the air vents on the floor.

Raised floor with one or more hot / cold aisles to direct airflow

Dual redundant UPS units. Assume %15-20 growth per year, design out for 5-7 years. Most large UPS units are modular, so you can add capacity later. Buy a larger unit that's got room to grow, (i.e. a 16kVA running at 8kVA) instead of buying a fully populated 8kVA unit.

Most equipment comes with dual redundant supplies. Run one supply to each UPS. Make sure you can shut down a whole UPS for service without an outage.

Invest in some nice PDUs. What good is a UPS if you have no idea how much power each leg is consuming? Make sure the PDU has an LCD so you can see how many amps you're drawing. It's nice being able to plug in equipment and being sure that it won't fail.

EPO buttons: make sure you wire your Emergency power off buttons to your supply panel so the whole room goes dark when they're hit. Battery supplied power should also be cut.

Fire suppression - As fuzz said, get a gas based system. Bring in a company that specializes in this. It will be worth the money for their expertise.

For rack space, assume the same growth rate. Leave room for extra racks.

Put cable trays above all the racks, make sure you can route cable from anywhere to anywhere without having a place with no tray.

Physical security - lockable racks, heavy duty doors, firewall above any drop ceilings, etc. As Fuzz said, RFID card locks are a good idea.

We didn't get good lighting. Get good lighting!

Lots of cable management, lots of velcro. Make it look nice, and go crazy on the number of connections. If you do it right, you won't have to touch anything for a few years. It will make your life a lot easier.





Full (four-post) cabinets for networking gear (w/doors). Separate from servers.
Server cabinets with front & back doors, two vertical PDUs w/ redundant power.
Power lines down into raised floor, data lines up to overhead trays.
Patch panels in each server cabinet (fibre and/or copper).
Use the cable management with proper-length data and power cables.
IPKVM everything including serial-to-IP for switch consoles, with management station/desk.
Color-code cables, but don't go crazy. Mgmt/KVM: yellow, LAN: grey, DMZ: red




Summary:

Environment
Properly spec'ed aircon. No leaks on the servers!
Humidity control
Fire prevention such as FM200
Optional: Ear protection for prolonged work in the area
Power and Network
UPS
Cable management instead of raised floors - think ladder racks
If two-post racks, get cable management for sides-of-rack. It's great!
Patch panels (from network equipment to top-of-rack)
Sysadmin
KVM station (Monitor/Keyboard/Mouse) for staging/troubleshooting
Preferable: Full workstation for staging and working in the room
Storage room for boxes (need room!), inventory and parts (bins and shelves). Trust me, you want this!
Supplies: Velcro, zipties, console cable, scissors, knife, etc. with a nice shelf for that next to the terminal station.
Whiteboard - Any sketches, notes, planning, or otherwise temporary documentation for the area.
Label Maker (bonus for one that can properly label cables)
Security
Ideal: Controlled, logged access to room via badges or passwords as well as keys if necessary
Acceptable: Strong locks
Video surveillance of outside of the door. Feed should at least get backed up outside the DC... what if the surveillance server gets jacked? Also, Data retention on that feed... you don't need more than two weeks worth of feed.
Console/ILO/root passwords on everything. Even if someone breaks in, they won't have access to anything. shut network ports, etc.
